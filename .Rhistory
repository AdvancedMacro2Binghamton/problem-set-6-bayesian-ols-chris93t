}
#We don't need a posterior
#A Proposal Function (Moving from one theta to another).. This one is the trickiest..
suggestion<-function(pars){
v<-sapply(c(std_errors,.00001), function (x) rnorm(1,0,x)) #Standard error residuals? hmm
pars_1<-pars+v
sd<-pars_1[length(pars)]
if(sd <= 0) {
pars_1<-pars
}
return(pars_1)
}
#A Metropolis Hastings Algorithem
#theta<-c(4.60198824,0.08781045,0.04112071,0.18374575 ,.1)  #Thera_0 intercept, beta 1, 2,3 and sd
theta<-c(10,.5,.7,.4,.5,.08 ,.37)
iterations<-10
values<-matrix(0,iterations,length(theta))
accept<-NULL
for (i in 1:iterations){
values[i,]<-theta
suggest<-suggestion(values[i,])
#print(suggest)
#print(values[i,])
#print(liklihood(suggest[1],suggest[2:6],suggest[7]))
#print(liklihood(values[i,1],values[i,2:6],values[i,7]))
ratio<-liklihood(suggest[1],suggest[2:6],suggest[7])-liklihood(values[i,1],values[i,2:6],values[i,7])
#ratio<-exp(ratio)
print(ratio)
u<-runif(1,0,1)
u<-log(u)
if(u<ratio & ratio!=0){  #We add this to reject everything where sigma < 0 ratio!=1
accept[i]<-TRUE
theta<-suggest
}
else{
accept[i]<-FALSE
theta<-theta
}
if(i > 1){
values_data<-as.data.frame(values[1:i,])
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/',i,'.png')
png(name)
Dist<-ggplot(data = values_data, aes(values[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
print(Dist)
dev.off()
}
}
}
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/','final','.png')
png(name)
Dist<-ggplot(data = values_data, aes(values[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
Dist<-Dist+geom_density(col='red4', size = 1)
print(Dist)
dev.off()
}
values_data
setwd('C:/Economics PhD/Third Year/Advanced Macro/HW6')
dir<-'C:/Economics PhD/Third Year/Advanced Macro/HW6/'
data<-read.csv('card.csv')
data<-data[,c('lwage','educ','exper','smsa', 'black', 'south')]
results<-lm(data$lwage~data$educ+data$exper+data$smsa+data$black+data$south)
#OLS Coefficients
print(results$coefficients)
std_errors<-summary(results)$coefficients[, "Std. Error"]
#OLS Residuals
sdev<-sd(results$residuals)
print(sdev)
labels<-c('beta wage','beta educ','beta exper','beta smsa', 'beta black', 'beta south','Sigma')
##Using the MCMC to estimate the posterior
#A flat prior Function
#A Liklihood function
liklihood<-function(int,betas,sd){
#This is obtained mathmatically by moving the epsilon which we assume is normally distributed on one side and so on. This is the assumption here
#Moving things around, epsilon becomes the regression equation which goes into the mean etc..
y_hat<-int + betas[1]*data$educ+betas[2]*data$exper+betas[3]*data$smsa+betas[4]*data$black+betas[5]*data$south
liklihood_result_1<-dnorm(data$lwage, mean = y_hat, sd = sd, log = T )
#This is wrong fix but commented out for now
#liklihood_result_1<-sapply(y_hat, function (x) dnorm(data$lwage, mean = x, sd = sd, log = T))
liklihood_result_2<-sum(liklihood_result_1)
return(liklihood_result_2)
}
#We don't need a posterior
#A Proposal Function (Moving from one theta to another).. This one is the trickiest..
suggestion<-function(pars){
v<-sapply(c(std_errors,.00001), function (x) rnorm(1,0,x)) #Standard error residuals? hmm
pars_1<-pars+v
sd<-pars_1[length(pars)]
if(sd <= 0) {
pars_1<-pars
}
return(pars_1)
}
#A Metropolis Hastings Algorithem
#theta<-c(4.60198824,0.08781045,0.04112071,0.18374575 ,.1)  #Thera_0 intercept, beta 1, 2,3 and sd
theta<-c(10,.5,.7,.4,.5,.08 ,.37)
iterations<-10
values<-matrix(0,iterations,length(theta))
accept<-NULL
for (i in 1:iterations){
values[i,]<-theta
suggest<-suggestion(values[i,])
#print(suggest)
#print(values[i,])
#print(liklihood(suggest[1],suggest[2:6],suggest[7]))
#print(liklihood(values[i,1],values[i,2:6],values[i,7]))
ratio<-liklihood(suggest[1],suggest[2:6],suggest[7])-liklihood(values[i,1],values[i,2:6],values[i,7])
#ratio<-exp(ratio)
print(ratio)
u<-runif(1,0,1)
u<-log(u)
if(u<ratio & ratio!=0){  #We add this to reject everything where sigma < 0 ratio!=1
accept[i]<-TRUE
theta<-suggest
}
else{
accept[i]<-FALSE
theta<-theta
}
#if(i > 1){
values_data<-as.data.frame(values[1:i,])
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/',i,'.png')
png(name)
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
print(Dist)
dev.off()
}
#}
}
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/','final','.png')
png(name)
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
Dist<-Dist+geom_density(col='red4', size = 1)
print(Dist)
dev.off()
}
as.data.frame(values[1:i,])
i
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist
j
values_data[,j]
values_data
i<-2
values_data<-as.data.frame(values[1:i,])
values_data
values_data[,j]
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist
print(Dist)
library(ggplot2)
setwd('C:/Economics PhD/Third Year/Advanced Macro/HW6')
dir<-'C:/Economics PhD/Third Year/Advanced Macro/HW6/'
data<-read.csv('card.csv')
data<-data[,c('lwage','educ','exper','smsa', 'black', 'south')]
results<-lm(data$lwage~data$educ+data$exper+data$smsa+data$black+data$south)
#OLS Coefficients
print(results$coefficients)
std_errors<-summary(results)$coefficients[, "Std. Error"]
#OLS Residuals
sdev<-sd(results$residuals)
print(sdev)
labels<-c('beta wage','beta educ','beta exper','beta smsa', 'beta black', 'beta south','Sigma')
##Using the MCMC to estimate the posterior
#A flat prior Function
#A Liklihood function
liklihood<-function(int,betas,sd){
#This is obtained mathmatically by moving the epsilon which we assume is normally distributed on one side and so on. This is the assumption here
#Moving things around, epsilon becomes the regression equation which goes into the mean etc..
y_hat<-int + betas[1]*data$educ+betas[2]*data$exper+betas[3]*data$smsa+betas[4]*data$black+betas[5]*data$south
liklihood_result_1<-dnorm(data$lwage, mean = y_hat, sd = sd, log = T )
#This is wrong fix but commented out for now
#liklihood_result_1<-sapply(y_hat, function (x) dnorm(data$lwage, mean = x, sd = sd, log = T))
liklihood_result_2<-sum(liklihood_result_1)
return(liklihood_result_2)
}
#We don't need a posterior
#A Proposal Function (Moving from one theta to another).. This one is the trickiest..
suggestion<-function(pars){
v<-sapply(c(std_errors,.00001), function (x) rnorm(1,0,x)) #Standard error residuals? hmm
pars_1<-pars+v
sd<-pars_1[length(pars)]
if(sd <= 0) {
pars_1<-pars
}
return(pars_1)
}
#A Metropolis Hastings Algorithem
#theta<-c(4.60198824,0.08781045,0.04112071,0.18374575 ,.1)  #Thera_0 intercept, beta 1, 2,3 and sd
theta<-c(10,.5,.7,.4,.5,.08 ,.37)
iterations<-10
values<-matrix(0,iterations,length(theta))
accept<-NULL
for (i in 1:iterations){
values[i,]<-theta
suggest<-suggestion(values[i,])
#print(suggest)
#print(values[i,])
#print(liklihood(suggest[1],suggest[2:6],suggest[7]))
#print(liklihood(values[i,1],values[i,2:6],values[i,7]))
ratio<-liklihood(suggest[1],suggest[2:6],suggest[7])-liklihood(values[i,1],values[i,2:6],values[i,7])
#ratio<-exp(ratio)
print(ratio)
u<-runif(1,0,1)
u<-log(u)
if(u<ratio & ratio!=0){  #We add this to reject everything where sigma < 0 ratio!=1
accept[i]<-TRUE
theta<-suggest
}
else{
accept[i]<-FALSE
theta<-theta
}
if(i > 1){
values_data<-as.data.frame(values[1:i,])
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/',i,'.png')
png(name)
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
print(Dist)
dev.off()
}
}
}
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/','final','.png')
png(name)
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
Dist<-Dist+geom_density(col='red4', size = 1)
print(Dist)
dev.off()
}
library(ggplot2)
setwd('C:/Economics PhD/Third Year/Advanced Macro/HW6')
dir<-'C:/Economics PhD/Third Year/Advanced Macro/HW6/'
data<-read.csv('card.csv')
data<-data[,c('lwage','educ','exper','smsa', 'black', 'south')]
results<-lm(data$lwage~data$educ+data$exper+data$smsa+data$black+data$south)
#OLS Coefficients
print(results$coefficients)
std_errors<-summary(results)$coefficients[, "Std. Error"]
#OLS Residuals
sdev<-sd(results$residuals)
print(sdev)
labels<-c('beta wage','beta educ','beta exper','beta smsa', 'beta black', 'beta south','Sigma')
##Using the MCMC to estimate the posterior
#A flat prior Function
#A Liklihood function
liklihood<-function(int,betas,sd){
#This is obtained mathmatically by moving the epsilon which we assume is normally distributed on one side and so on. This is the assumption here
#Moving things around, epsilon becomes the regression equation which goes into the mean etc..
y_hat<-int + betas[1]*data$educ+betas[2]*data$exper+betas[3]*data$smsa+betas[4]*data$black+betas[5]*data$south
liklihood_result_1<-dnorm(data$lwage, mean = y_hat, sd = sd, log = T )
#This is wrong fix but commented out for now
#liklihood_result_1<-sapply(y_hat, function (x) dnorm(data$lwage, mean = x, sd = sd, log = T))
liklihood_result_2<-sum(liklihood_result_1)
return(liklihood_result_2)
}
#We don't need a posterior
#A Proposal Function (Moving from one theta to another).. This one is the trickiest..
suggestion<-function(pars){
v<-sapply(c(std_errors,.00001), function (x) rnorm(1,0,x)) #Standard error residuals? hmm
pars_1<-pars+v
sd<-pars_1[length(pars)]
if(sd <= 0) {
pars_1<-pars
}
return(pars_1)
}
#A Metropolis Hastings Algorithem
#theta<-c(4.60198824,0.08781045,0.04112071,0.18374575 ,.1)  #Thera_0 intercept, beta 1, 2,3 and sd
theta<-c(10,.5,.7,.4,.5,.08 ,.37)
iterations<-1000
values<-matrix(0,iterations,length(theta))
accept<-NULL
for (i in 1:iterations){
values[i,]<-theta
suggest<-suggestion(values[i,])
#print(suggest)
#print(values[i,])
#print(liklihood(suggest[1],suggest[2:6],suggest[7]))
#print(liklihood(values[i,1],values[i,2:6],values[i,7]))
ratio<-liklihood(suggest[1],suggest[2:6],suggest[7])-liklihood(values[i,1],values[i,2:6],values[i,7])
#ratio<-exp(ratio)
print(ratio)
u<-runif(1,0,1)
u<-log(u)
if(u<ratio & ratio!=0){  #We add this to reject everything where sigma < 0 ratio!=1
accept[i]<-TRUE
theta<-suggest
}
else{
accept[i]<-FALSE
theta<-theta
}
if(i > 1){
values_data<-as.data.frame(values[1:i,])
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/',i,'.png')
png(name)
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
print(Dist)
dev.off()
}
}
}
for (j in 1:length(labels)){
name<-paste0(dir,labels[j],'/','final','.png')
png(name)
Dist<-ggplot(data = values_data, aes(values_data[,j]))+geom_histogram(aes(y =..density..),fill="green4", col = 'black')
Dist<-Dist+theme_minimal()+theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())
Dist<-Dist+theme(plot.title = element_text(hjust = 0.5),axis.title.y=element_text(margin=margin(0,25,0,0)),axis.title.x =element_text(margin=margin(15,0,0,0)))
Dist<-Dist+ggtitle('')+theme(plot.title = element_text(hjust = 0.5), legend.position=c(.2,.8) )
Dist<-Dist+scale_x_continuous(name = labels[j])+scale_y_continuous(name='')+guides(fill=guide_legend(title=" "))
Dist<-Dist+geom_density(col='red4', size = 1)
print(Dist)
dev.off()
}
labels<-c('beta wage','beta educ','beta exper','beta smsa', 'beta black', 'beta south','Sigma')
##Using the MCMC to estimate the posterior
#A flat prior Function
#A Liklihood function
liklihood<-function(int,betas,sd){
#This is obtained mathmatically by moving the epsilon which we assume is normally distributed on one side and so on. This is the assumption here
#Moving things around, epsilon becomes the regression equation which goes into the mean etc..
y_hat<-int + betas[1]*data$educ+betas[2]*data$exper+betas[3]*data$smsa+betas[4]*data$black+betas[5]*data$south
liklihood_result_1<-dnorm(data$lwage, mean = y_hat, sd = sd, log = T )
#This is wrong fix but commented out for now
#liklihood_result_1<-sapply(y_hat, function (x) dnorm(data$lwage, mean = x, sd = sd, log = T))
liklihood_result_2<-sum(liklihood_result_1)
return(liklihood_result_2)
}
#We don't need a posterior
#A Proposal Function (Moving from one theta to another).. This one is the trickiest..
suggestion<-function(pars){
v<-sapply(c(std_errors,.00001), function (x) rnorm(1,0,x)) #Standard error residuals? hmm
pars_1<-pars+v
sd<-pars_1[length(pars)]
if(sd <= 0) {
pars_1<-pars
}
return(pars_1)
}
#A Metropolis Hastings Algorithem
#theta<-c(4.60198824,0.08781045,0.04112071,0.18374575 ,.1)  #Thera_0 intercept, beta 1, 2,3 and sd
theta<-c(10,.5,.7,.4,.5,.08 ,.37)
iterations<-3000
values<-matrix(0,iterations,length(theta))
accept<-NULL
for (i in 1:iterations){
values[i,]<-theta
suggest<-suggestion(values[i,])
#print(suggest)
#print(values[i,])
#print(liklihood(suggest[1],suggest[2:6],suggest[7]))
#print(liklihood(values[i,1],values[i,2:6],values[i,7]))
ratio<-liklihood(suggest[1],suggest[2:6],suggest[7])-liklihood(values[i,1],values[i,2:6],values[i,7])
#ratio<-exp(ratio)
print(ratio)
u<-runif(1,0,1)
u<-log(u)
if(u<ratio & ratio!=0){  #We add this to reject everything where sigma < 0 ratio!=1
accept[i]<-TRUE
theta<-suggest
}
else{
accept[i]<-FALSE
theta<-theta
}
}
labels<-c('beta wage','beta educ','beta exper','beta smsa', 'beta black', 'beta south','Sigma')
liklihood<-function(int,betas,sd){
#This is obtained mathmatically by moving the epsilon which we assume is normally distributed on one side and so on. This is the assumption here
#Moving things around, epsilon becomes the regression equation which goes into the mean etc..
y_hat<-int + betas[1]*data$educ+betas[2]*data$exper+betas[3]*data$smsa+betas[4]*data$black+betas[5]*data$south
liklihood_result_1<-dnorm(data$lwage, mean = y_hat, sd = sd, log = T )
#This is wrong fix but commented out for now
#liklihood_result_1<-sapply(y_hat, function (x) dnorm(data$lwage, mean = x, sd = sd, log = T))
liklihood_result_2<-sum(liklihood_result_1)
return(liklihood_result_2)
}
labels<-c('beta wage','beta educ','beta exper','beta smsa', 'beta black', 'beta south','Sigma')
liklihood<-function(int,betas,sd){
#This is obtained mathmatically by moving the epsilon which we assume is normally distributed on one side and so on. This is the assumption here
#Moving things around, epsilon becomes the regression equation which goes into the mean etc..
y_hat<-int + betas[1]*data$educ+betas[2]*data$exper+betas[3]*data$smsa+betas[4]*data$black+betas[5]*data$south
liklihood_result_1<-dnorm(data$lwage, mean = y_hat, sd = sd, log = T )
#This is wrong fix but commented out for now
#liklihood_result_1<-sapply(y_hat, function (x) dnorm(data$lwage, mean = x, sd = sd, log = T))
liklihood_result_2<-sum(liklihood_result_1)
return(liklihood_result_2)
}
suggestion<-function(pars){
v<-sapply(c(std_errors,.00001), function (x) rnorm(1,0,x)) #Standard error residuals? hmm
pars_1<-pars+v
sd<-pars_1[length(pars)]
if(sd <= 0) {
pars_1<-pars
}
return(pars_1)
}
ibrary(ggplot2)
ibrary(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
ibrary(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
setwd('C:/Economics PhD/Third Year/Advanced Macro/HW6')
dir<-'C:/Economics PhD/Third Year/Advanced Macro/HW6/'
data<-read.csv('card.csv')
data<-data[,c('lwage','educ','exper','smsa', 'black', 'south')]
results<-lm(data$lwage~data$educ+data$exper+data$smsa+data$black+data$south)
#OLS Coefficients
print(results$coefficients)
std_errors<-summary(results)$coefficients[, "Std. Error"]
#OLS Residuals
sdev<-sd(results$residuals)
print(sdev)
library(ggplot2)
setwd('C:/Economics PhD/Third Year/Advanced Macro/HW6')
dir<-'C:/Economics PhD/Third Year/Advanced Macro/HW6/'
data<-read.csv('card.csv')
data<-data[,c('lwage','educ','exper','smsa', 'black', 'south')]
results<-lm(data$lwage~data$educ+data$exper+data$smsa+data$black+data$south)
#OLS Coefficients
print(results$coefficients)
std_errors<-summary(results)$coefficients[, "Std. Error"]
#OLS Residuals
sdev<-sd(results$residuals)
labels<-c('beta wage','beta educ','beta exper','beta smsa', 'beta black', 'beta south','Sigma')
##Using the MCMC to estimate the posterior
#A flat prior Function
#A Liklihood function
liklihood<-function(int,betas,sd){
#This is obtained mathmatically by moving the epsilon which we assume is normally distributed on one side and so on. This is the assumption here
#Moving things around, epsilon becomes the regression equation which goes into the mean etc..
y_hat<-int + betas[1]*data$educ+betas[2]*data$exper+betas[3]*data$smsa+betas[4]*data$black+betas[5]*data$south
liklihood_result_1<-dnorm(data$lwage, mean = y_hat, sd = sd, log = T )
#This is wrong fix but commented out for now
#liklihood_result_1<-sapply(y_hat, function (x) dnorm(data$lwage, mean = x, sd = sd, log = T))
liklihood_result_2<-sum(liklihood_result_1)
return(liklihood_result_2)
}
#We don't need a posterior
#A Proposal Function (Moving from one theta to another).. This one is the trickiest..
suggestion<-function(pars){
v<-sapply(c(std_errors,.00001), function (x) rnorm(1,0,x)) #Standard error residuals? hmm
pars_1<-pars+v
sd<-pars_1[length(pars)]
if(sd <= 0) {
pars_1<-pars
}
return(pars_1)
}
#A Metropolis Hastings Algorithem
#theta<-c(4.60198824,0.08781045,0.04112071,0.18374575 ,.1)  #Thera_0 intercept, beta 1, 2,3 and sd
theta<-c(10,.5,.7,.4,.5,.08 ,.37)
iterations<-3000
values<-matrix(0,iterations,length(theta))
accept<-NULL
for (i in 1:iterations){
values[i,]<-theta
suggest<-suggestion(values[i,])
#print(suggest)
#print(values[i,])
#print(liklihood(suggest[1],suggest[2:6],suggest[7]))
#print(liklihood(values[i,1],values[i,2:6],values[i,7]))
ratio<-liklihood(suggest[1],suggest[2:6],suggest[7])-liklihood(values[i,1],values[i,2:6],values[i,7])
#ratio<-exp(ratio)
print(ratio)
u<-runif(1,0,1)
u<-log(u)
if(u<ratio & ratio!=0){  #We add this to reject everything where sigma < 0 ratio!=1
accept[i]<-TRUE
theta<-suggest
}
else{
accept[i]<-FALSE
theta<-theta
}
}
